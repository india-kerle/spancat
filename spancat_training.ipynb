{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83467139",
   "metadata": {},
   "source": [
    "Welcome to the code behind Nesta's medium article, \"Training spaCy's spancat component in Python\"! üëã\n",
    "\n",
    "Although spaCy pushes for config-based training, sometimes you simply want to experiment with a component in a jupyter notebook without the headache (and commitment) of setting everything up. And although there is plenty online on how to train a custom NER model in spaCy, there is virtually nothing on how to do the same for a custom spancat model.\n",
    "\n",
    "Training a custom spancat model looks very similar to training a custom NER model but has two key differences: \n",
    "\n",
    "1. the training data needs to be in a different format and; \n",
    "2. you must convert your training data to a list of spaCy Examples to train the model.\n",
    "\n",
    "This notebook will walk you through training a custom spancat model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454f467",
   "metadata": {},
   "source": [
    "#### 0. Load Libraries and parameters\n",
    "let's first import the libraries and parameters we will need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162d1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import string \n",
    "from spacy import displacy\n",
    "\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from spacy.pipeline.spancat import DEFAULT_SPANCAT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71ab98",
   "metadata": {},
   "source": [
    "we're going to name the key to store our labelled spans \"sc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed27298",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_key = \"sc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fef01",
   "metadata": {},
   "source": [
    "#### 1. Load Data\n",
    "Once we've loaded the relevant libraries and parameters, lets download and clean up our data. The README.md tells you how to download the data üíæ.\n",
    "\n",
    "We want to extract people, organisation and locations tags. So, we can just replace the other tags in the data we don't care about with 'O'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791cdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and fill nans with sentence #s\n",
    "ner_data = (pd.read_csv(\"ner_datasetreference.csv\", encoding='ISO-8859-1')\n",
    "            .fillna(method='ffill'))\n",
    "\n",
    "#replace tags we don't care about with 'O'\n",
    "ents_to_replace = ['tim', 'gpe', 'art', 'eve', 'nat']\n",
    "bad_ents = []\n",
    "for ent in list(set(ner_data.Tag)):\n",
    "    if any(ent.endswith(e) for e in ents_to_replace):\n",
    "        bad_ents.append(ent)\n",
    "        \n",
    "ner_data = ner_data.replace(bad_ents, 'O')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5349a",
   "metadata": {},
   "source": [
    "#### 2. Format data for spaCy\n",
    "Now that we've loaded and replaced the non-relevant tags with 'O', let's format the data in a way that spaCy can handle. We will need to get the start and end of spans in the text. \n",
    "\n",
    "We will do this with the get_span_indx function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9956f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_indx(\n",
    "    labels: List[str]\n",
    ") -> List[tuple]:\n",
    "    \"\"\"Gets span starts and ends for Spacy spancat component.\n",
    "        \n",
    "        Returns list of tuples where the first element of the \n",
    "        tuple is the span start, the second element of the tuple\n",
    "        is the span end and the third element of the tuple is\n",
    "        the span category. \n",
    "    \"\"\"\n",
    "    label_indx = []\n",
    "    temp_list = []\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != 'O':\n",
    "            temp_list.append(i)\n",
    "        else:\n",
    "            label_indx.append(temp_list)\n",
    "            temp_list = []    \n",
    "        if i == len(labels) - 1:\n",
    "            label_indx.append(temp_list)\n",
    "\n",
    "    clean_label_indx = [x for x in label_indx if len(x) > 0]\n",
    "    \n",
    "    spans = []\n",
    "    for indx in clean_label_indx:\n",
    "        if len(indx) == 1:\n",
    "            spans.append((indx[0], indx[0] + 1))\n",
    "        else:\n",
    "            spans.append((indx[0], indx[-1] + 1))\n",
    "\n",
    "    return [(s[0], s[1], labels[s[0]].split('-')[-1].upper()) for s in spans]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59f7f9",
   "metadata": {},
   "source": [
    "Once we've written a function to extract span start and end indices, we will format the training data for spaCy such that:\n",
    "\n",
    "```\n",
    "  ('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country',\n",
    "  {'spans': {'sc': [(6, 7, 'GEO'), (12, 13, 'GEO')]}})\n",
    "```\n",
    "\n",
    "Where the first element of the tuple is the text and the second element of the tuple is a nested dictionary where the values of the span_key (in this instance, \"sc\") is a list of tuples containing the span token start, span token end and span category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d40a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spaCy compliant training data \n",
    "train_data = []\n",
    "for sent, sent_info in ner_data.groupby(\"Sentence #\"):\n",
    "    sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(sent_info[\"Word\"]))\n",
    "    sentence_no_punct = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    labels = list(sent_info['Tag'])\n",
    "    span_ents = get_span_indx(labels)\n",
    "    train_data.append((sentence_no_punct, {'spans':{span_key: [(span_ent[0], span_ent[1], span_ent[2]) for span_ent in span_ents]}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51446a0a",
   "metadata": {},
   "source": [
    "#### 3. Train spancat component with labelled entities\n",
    "\n",
    "Now that we've formatted the training data in such a way to be able to train the spancat component, let's instantiate a blank spaCy object and add our labels to the component üè∑Ô∏è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1788095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate blank spaCy object\n",
    "nlp = spacy.blank('en')\n",
    "config = {\n",
    "    #minimum confidence threshold \n",
    "    \"threshold\": 0.2,\n",
    "    \"spans_key\": span_key,\n",
    "    \"max_positive\": None,\n",
    "    \"model\": DEFAULT_SPANCAT_MODEL,\n",
    "    #we're suggesting fixed n-gram length of up to 3 tokens\n",
    "    \"suggester\": {\"@misc\": \"spacy.ngram_suggester.v1\", \"sizes\": [1, 2, 3]},\n",
    "}\n",
    "nlp.add_pipe(\"spancat\", config=config)\n",
    "span=nlp.get_pipe('spancat')\n",
    "\n",
    "#Add labels to spancat component \n",
    "for label in ('GEO', 'PER', 'ORG'):\n",
    "    span.add_label(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde47ed9",
   "metadata": {},
   "source": [
    "Now that we've done that, let's get the pipe we want to train on, initialise the spacy object and create an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c82869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pipe you want to train on \n",
    "pipe_exceptions = [\"spancat\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# initialise spacy object \n",
    "nlp.initialize()\n",
    "sgd = nlp.create_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70008ad9",
   "metadata": {},
   "source": [
    "...and let the training begin! ‚è≤Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c940c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [06:32<00:00, 13.08s/it]\n"
     ]
    }
   ],
   "source": [
    "#start training spancat component \n",
    "all_losses = []\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for iteration in tqdm(range(30)):\n",
    "        # shuffling examples before every iteration\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        batch_examples = []\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            examples = Example.from_dict(nlp.make_doc(texts[0]), annotations[0])\n",
    "            batch_examples.append(examples)\n",
    "        #nlp.update for spacy component takes list of examples \n",
    "        nlp.update(batch_examples, losses=losses, drop=0.1, sgd=sgd)\n",
    "        all_losses.append(losses['spancat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a090ac6",
   "metadata": {},
   "source": [
    "We can investigate the losses by printing all_losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c72febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157150.109375\n",
      "84768.9296875\n",
      "41765.94921875\n",
      "12769.548828125\n",
      "2690.6083984375\n",
      "448.4676818847656\n",
      "158.1262969970703\n",
      "108.01145935058594\n",
      "106.51181030273438\n",
      "83.88938903808594\n",
      "47.929771423339844\n",
      "31.802108764648438\n",
      "50.90104293823242\n",
      "43.02567672729492\n",
      "33.061344146728516\n",
      "30.267887115478516\n",
      "30.3626708984375\n",
      "37.78573989868164\n",
      "27.46986961364746\n",
      "35.77145004272461\n",
      "34.98078155517578\n",
      "29.197343826293945\n",
      "29.09355354309082\n",
      "23.123306274414062\n",
      "41.04826736450195\n",
      "26.509939193725586\n",
      "41.246124267578125\n",
      "32.0553092956543\n",
      "37.610897064208984\n",
      "34.841835021972656\n"
     ]
    }
   ],
   "source": [
    "for loss in all_losses:\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b578e",
   "metadata": {},
   "source": [
    "#### 4. Apply the custom trained spancat model \n",
    "\n",
    "Now that we've trained our model, lets apply it to a news article sentence and see what spans it predicts as people, organisations and locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "409c4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on sentence from the BBC news article yesterday\n",
    "text = \"Yes Liz Truss would deliver the governments 2050 net zero target in the UK and france but the prime minister also restated her determination to issue more oil and gas licences in the North Sea\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987796de",
   "metadata": {},
   "source": [
    "We're able to print the predicted spans and span-level confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b4fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print predicted spans and entity level scores\n",
    "spans = doc.spans[span_key]\n",
    "for span, confidence in zip(spans, spans.attrs[\"scores\"]):\n",
    "    print(span.label_, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8c1e6",
   "metadata": {},
   "source": [
    "Finally, we can use spaCy's `displacy` to show the predicted spans in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d60887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"spans\" style=\"line-height: 2.5; direction: ltr\">Yes Liz Truss would deliver the governments 2050 net zero target in the UK and france but the prime minister also restated her determination to issue more oil and gas licences in the North Sea </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show predicted spans \n",
    "displacy.render(doc, style=\"span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b333a",
   "metadata": {},
   "source": [
    "And that's that! If you'd like to learn more about the spancat architecture üèóÔ∏è and a bit more detail about the code, do refer back to the medium article. This is purely demonstrative so we're not evaluating the model, tuning hyperparameters, playing around with the suggester or using a spans-specific dataset.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skills-taxonomy-v2",
   "language": "python",
   "name": "skills-taxonomy-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
